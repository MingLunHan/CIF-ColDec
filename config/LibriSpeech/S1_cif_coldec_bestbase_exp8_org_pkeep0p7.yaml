---
task: "stl_hvd_train"
num_gpus: 1
exp_basedir: "/opt/tiger/CIF/egs/librispeech/exp"

model:
    model_name: "cif_coldec"
    vocab_size: 3726
    char_vocab_path: "./data/librispeech_80fbank_token_end2end_coldec_stl_asr/org_data/token_id.txt"
    pinyin_vocab_path: "./data/librispeech_80fbank_token_end2end_coldec_stl_asr/org_data/py_token_id.txt"

    # front end settings
    use_cnn_frontend: True
    conv_norm_type: "layer_correct"
    down_sample_conv_num_layers: 1
    down_sample_conv_num_filters: 64
    additional_module: "new_mu33_2"
    additional_module_num_layers: 1
    additional_module_num_filters: 64
    num_frame_stacking: 3
    num_frame_striding: 3

    # self-attention encoder/decoder setting
    self_attention_type: "dot_product"
    num_heads: 8
    hidden_size: 512
    filter_size: 2048
    ffn_layer: "dense_relu_dense"
    layer_preprocess_sequence: "n"
    layer_postprocess_sequence: "da"
    norm_type: "layer"
    norm_epsilon: 0.000001
    pos: None
    proximity_bias: True

    attention_key_channels:
    attention_value_channels:
    parameter_attention_key_channels:
    parameter_attention_value_channels:
    max_relative_position: 0

    num_encoder_layers: 17
    encoder_self_attention_type: "dot_product"
    sa_pooling_layers: [5, 10]

    num_decoder_layers: 2
    decoder_self_attention_type: "dot_product"

    use_conformer_encoder: True
    conformer_conv_width: 15
    use_sin_rel_dot_product: True
    conformer_add_pad_mask: True
    rel_pos_version: "3"

    # use auto-regressive (ar) /non-auto-regressive (nar) decoder
    use_teacher_forcing: True
    use_pss: False
    pss_rate: 0.0

    # chunk hopping
    use_chunk_hopping: False
    chunk_size: 128
    hop_size: 64
    ch_type: "middle"
    ch_fixed_future_size: 32
    load_trained_offline_model: True
    ch_cif_dir: "./exp/librispeech_80fbank_token_end2end_stl_asr/cif_base_exp1/models"

    # regularizer
    layer_prepostprocess_dropout: 0.1
    attention_dropout: 0.0
    relu_dropout: 0.0

    # extra
    loss_multiplier: 2.0
    shared_embedding_and_softmax_weights: False
    multiply_embedding_mode: "sqrt_depth"
    symbol_modality_num_shards: 1

    # cif part
    produce_weights_type: "conv"
    conv_cif_num_layers: 1
    conv_cif_width_string: "3"
    conv_cif_num_filters: 512
    conv_cif_dropout: 0.1
    dense_cif_units: 512

    cif_weight_threshold: 0.99999
    use_scaling_strategy: True
    use_tail_handling: True
    add_eos_to_target: False

    # all loss setting
    calculated_loss: "contextual_ce_loss, quantity_loss, ctc_loss_on_encoder"

    # ce loss
    label_smoothing: 0.2
    ls_type: "uniform"

    # confidence penalty
    using_confidence_penalty: False
    cp_lambda: 0.0

    # ctc loss on the encoder
    ctc_loss_on_encoder: True
    ctc_loss_lambda: 0.0

    # quantity loss
    quantity_loss_lambda: 0.0

    # span alignment loss
    span_alignment_loss_lambda: 0.0
    sal_end_training_step_num:

    # token contextual ce loss
    token_contextual_ce_loss_lambda: 0.0
    tccl_start_training_step_num:

    # Contextual biasing settings
    use_coldec: True
    phrases_type: "text"
    contextual_content: "1gram,2gram,3gram,4gram"
    num_c_batch: 8
    average_mode: "global"
    max_phrase_len: 30
    ctxt_text_vocab_size: 3728
    ctxt_pinyin_vocab_size: 3728
    disable_contextual_phrases_generation_op: False
    disable_contextual_phrases_uniq_op: False

    use_no_bias_option: True
    train_used_ngram_per_utt: 1
    train_p_keep: 0.7
    eval_used_ngram_per_utt: 1

    # External phrases settings
    use_specific_phrases: False
    specific_phrases_path: "/opt/tiger/CIF/egs/librispeech/data/librispeech_80fbank_token_end2end_coldec_stl_asr/org_data/hotwords.txt"
    use_random_phrases: False
    random_phrases_path: ""
    use_utt_specific_phrases: False
    utt_specific_phrases_path: ""

    # CPN encoder settings
    direct_predict: False
    contextual_encoder: "san"
    num_ctxt_encoder_layers: 4
    ctxt_num_heads: 8
    nar_decoder_use_triangle_bias: True
    ctxt_encoder_hidden_size: 512
    ctxt_encoder_filter_size: 2048
    ctxt_encoder_self_attention_type: "dot_product"
    ctxt_encoder_position_encoding_type: ""
    ctxt_layer_prepostprocess_dropout: 0.0
    ctxt_attention_dropout: 0.0

    # CPN decoder settings
    num_ctxt_decoder_layers: 2

    # context-augmented query
    use_augmented_history: False
    augmented_history_length: 3
    augmented_query_integration_type: "concat_proj"

    # CPN attention settings
    simple_ctxt_attention: False
    ctxt_attention_layer: 1
    integration_mode: "concat"
    ctxt_inputs_query_text: True
    ctxt_inputs_query_pinyin: False

    # Token-level attention settings
    use_token_attention: False
    token_attention_topk_filter_mode: "global"
    token_level_query: "cif_outputs"
    screened_phrase_num: 5
    use_integrated_token_embedding: True
    concat_phrase_embeddings: False
    add_phrase_embeddings: True
    token_attention_outputs_comb_location: "decoder_outputs"
    token_embedding_fusion_type: "concat"
    token_attention_type: "simple_MHA"
    ctxt_token_attention_layer: 1
    force_add_no_bias_option: True

    # SpecAug
    use_specaugment: True
    F: 27
    T: 20
    mF: 1
    mT: 10
    p: 1.0
    new_specaugment: True
    ps: 0.03

    # Joint decoding settings
    joint_ctc_cif_decoding: False
    joint_decoding_lambda: 0.0
    joint_only_ctc: False
    joint_ctc_temperature: 1.0
    joint_cif_temperature: 0.8

    # Collaborative Decoding settings
    joint_ctxt_cif_decoding: True
    joint_ctxt_temperature: 1.0
    only_ctxt_greedy_decoding: False
    coldec_lambda: 0.5
    topk_renormalize: False
    renormalize_topk_num: 10
    use_asi: False

    # Use pretrained cif main body
    use_pretrained_cif_body: True
    pretrained_cif_body_model_path: "/opt/tiger/CIF/egs/librispeech/exp/librispeech_80fbank_token_end2end_stl_asr/cif_best_model/models"

    # Freezing settings
    freeze_specific_variables: True
    freeze_contextual_encoder: False

    use_fp16: True
    use_hvd_optimizer: True
    use_hvd_Adasum: False

    # log and print settings
    log_and_see: False
#    print_attention_weights: True
#    print_inserted_phrases: True

    # Enable/Disable utterance id
    use_utter_id: False

train:
    # optimizing relevants
#    optimizer: "lazy_adam_decay"
#    learning_rate: 0.4
#    warmup_steps: 36000
#    grads_clip: 1.0

    optimizer: "adam_plateau_lazy"
    weight_decay: 0.00001
    stop_warmup_step: 1000
    start_decay_step: 20000
    end_decay_step: 40000
    each_decay_step: 4000
    peak_lr: 0.001
    init_lr: 0
    shrink_time: 0.1

    learning_rate: 0.001
    warmup_steps: 1000
    grads_clip: 1.0

    # evaluation setting
    text_decode_func: "ag20k_cif_text_decode"
    beam_size: 1
    top_beams: 1

    # gradient calculation
    used_loss_names: "contextual_ce_loss, quantity_loss, ctc_loss_on_encoder"
    summary_freq: 100
    logging_freq: 100

    # training process control
    early_stop_metric: "wer"

    metric_not_grow_limits: 8000
#    save_checkpoint_freq: 10
    save_checkpoint_freq: 800
    total_train_step: 80000
    just_eval: False

    # extra
    write_results: True
    eval_on_dev: True
    use_daisy_chain_getter: False
    max_saved_checkpoints: 20

    # gradient accumulation
    use_grad_accumulation: True
    accumulation_step: 6

eval:
    average_ckpts: True
    num_averaging_ckpts: 10

    # evaluation setting
    text_decode_func: "ag20k_cif_text_decode"
    beam_size: 10
#    beam_size: 1
    top_beams: 1

    output_probs: False
    not_show_results: False

data:
    # data generator configs
    tfrecord_on_hdfs: True
    hdfs_data_dir: ""
    eval_on_hdfs: False
    # if data not on hdfs
    data_basedir: "/opt/tiger/CIF/egs/librispeech/data"

    num_shards: 32
    clear_existed: False

    # data reader configs
    data_type: "asr_data_with_ngrams_with_uttid"
#    data_type: "asr_data_with_ngrams"
    scp_feat_total_dim: 80
    scp_feat_channel_num: 1
    scp_feat_use_delta: False
    max_input_seq_length:
    max_target_seq_length:
    preprocess: True
    max_expected_batch_size_per_gpu: 2048

    frms_per_train_batch: 40000
    frms_per_eval_batch: 30000
    train_boundaries: "690,952,1116,1212,1274,1319,1355,1385,1411,1435,1456,1476,1496,1514,1533,1551,1569,1588,1617"
    dev_boundaries: "690,952,1116,1212,1274,1319,1355,1385,1411,1435,1456,1476,1496,1514,1533,1551,1569,1588,1617"
    eval_boundaries: "690,952,1116,1212,1274,1319,1355,1385,1411,1435,1456,1476,1496,1514,1533,1551,1569,1588,1617"
    max_length: 2000
    min_length: 100

    max_phrase_len: 30

    # in order to prevent OOM, directly pointing the batch_size of last bucket
    pointed_last_bucket_bs:

    # accelerate the speed of data reading
    faster_hdfs_reading: True
    hvd_reading: True
